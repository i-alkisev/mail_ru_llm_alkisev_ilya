{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "10bf3438",
      "metadata": {
        "id": "10bf3438"
      },
      "source": [
        "# Ассистент 1 - LM на основе n-грамм"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6de848df",
      "metadata": {
        "id": "6de848df"
      },
      "source": [
        "Перед вами - первое дополнительное задание повышенной сложности, в рамках которого вам предстоит начать разработку телеграм-бота с генеративными моделями.\n",
        "\n",
        "Цель данного ноутбука - помочь влиться в разработку ассистента. В данном ноутбуке написан код для \"обучения\" LM на основе n-грамм, для генерации с помощью нее текста, а также сохранение и загрузка модели и токенизатора.\n",
        "\n",
        "Относитесь к данному заданию максимально творчески - любую часть кода можно менять под ваши нужды и желания, можно оптимизировать, добавлять методы генерации, использовать любые данные, обучать сколь угодно \"большую\" модель.\n",
        "\n",
        "При этом вам стоит быть готовыми со всеми техническими проблеми справляться самому - именно так обычно происходит в реальной жизни в реальных проектах :)\n",
        "\n",
        "Поэтому отдельно подчеркну:\n",
        "* если что-то сломалось после ваших изменений - подразумевается, что вы сами найдете проблему и исправите\n",
        "* если вы ничего не трогали, но что-то не работает у нас - подразумевается, что вы сами найдете проблему и исправите :)\n",
        "\n",
        "Главный критерий выполнености данного задания - телеграм-бот, генерирующий текст и использующий обозначенный в задании подход (в случае данного ноутбука - n-граммная модель в любой ее реализации)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aecb8bd4",
      "metadata": {
        "id": "aecb8bd4"
      },
      "source": [
        "_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31ca60e8",
      "metadata": {
        "id": "31ca60e8"
      },
      "source": [
        "Для обучения качественной модели вам потребуются датасеты. В ноутбуке составлен маленький игрушечный датасет, вам для улучшения качества потребуется данные в большем количестве и более качественные, а также другие параметры модели и генерации (например, размер контекста побольше).\n",
        "\n",
        "С нормальным датасетом и правильными параметрами даже такой простой моделью можно добиться адекватного качества генерации текста (возможно не очень человечный, но вполне связный текст).\n",
        "\n",
        "Датасеты можно найти и выбрать тут (желательно на русском, вам так будет понятней качество и в целом полезней):\n",
        "https://huggingface.co/datasets\n",
        "  \n",
        "Можете найти наиболее интересный для себя датасет (можете сделать модель как смешной, так и полезной), либо выбрать любой из этих датасетов\n",
        "* https://huggingface.co/datasets/Den4ikAI/russian_dialogues\n",
        "* https://huggingface.co/datasets/Georgii/russianPoetry\n",
        "* https://huggingface.co/datasets/IgorVolochay/russian_jokes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t38DDOOSdbh",
        "outputId": "6c919e2d-f72e-4f2a-e868-dc334ddb9c79"
      },
      "id": "0t38DDOOSdbh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c6c33a9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T14:50:17.414585Z",
          "iopub.status.busy": "2024-02-21T14:50:17.413838Z",
          "iopub.status.idle": "2024-02-21T14:50:37.187697Z",
          "shell.execute_reply": "2024-02-21T14:50:37.186316Z",
          "shell.execute_reply.started": "2024-02-21T14:50:17.414553Z"
        },
        "id": "0c6c33a9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pickle\n",
        "from itertools import chain\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "from typing import List, Dict, Optional, Iterable, Tuple\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import tokenizers\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.processors import TemplateProcessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df809daa",
      "metadata": {
        "id": "df809daa"
      },
      "source": [
        "Токенизатор разбивает текст на слова. Можно попробовать другие способы токенизации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90160389",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T15:03:11.789756Z",
          "iopub.status.busy": "2024-02-21T15:03:11.788899Z",
          "iopub.status.idle": "2024-02-21T15:03:11.808584Z",
          "shell.execute_reply": "2024-02-21T15:03:11.807519Z",
          "shell.execute_reply.started": "2024-02-21T15:03:11.789725Z"
        },
        "id": "90160389"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self,\n",
        "                 token_pattern: str = '\\w+|[\\!\\?\\,\\.\\-\\:]',\n",
        "                 eos_token: str = '<EOS>',\n",
        "                 pad_token: str = '<PAD>',\n",
        "                 unk_token: str = '<UNK>'):\n",
        "        self.token_pattern = token_pattern\n",
        "        self.eos_token = eos_token\n",
        "        self.pad_token = pad_token\n",
        "        self.unk_token = unk_token\n",
        "\n",
        "        self.special_tokens = [self.eos_token, self.pad_token, self.unk_token]\n",
        "        self.vocab = None\n",
        "        self.inverse_vocab = None\n",
        "\n",
        "    def text_preprocess(self, input_text: str) -> str:\n",
        "        \"\"\" Предобрабатываем один текст \"\"\"\n",
        "        input_text = input_text.lower()\n",
        "        input_text = re.sub('\\s+', ' ', input_text) # унифицируем пробелы\n",
        "        input_text = input_text.strip()\n",
        "        return input_text\n",
        "\n",
        "    def build_vocab(self, corpus: List[str]) -> None:\n",
        "        assert len(corpus)\n",
        "        all_tokens = set()\n",
        "        for text in corpus:\n",
        "            all_tokens |= set(self._tokenize(text, append_eos_token=False))\n",
        "        self.vocab = {elem: ind for ind, elem in enumerate(all_tokens)}\n",
        "        special_tokens = [self.eos_token, self.unk_token, self.pad_token]\n",
        "        for token in special_tokens:\n",
        "            self.vocab[token] = len(self.vocab)\n",
        "        self.inverse_vocab = {ind: elem for elem, ind in self.vocab.items()}\n",
        "        return self\n",
        "\n",
        "    def _tokenize(self, text: str, append_eos_token: bool = True) -> List[str]:\n",
        "        text = self.text_preprocess(text)\n",
        "        tokens = re.findall(self.token_pattern, text)\n",
        "        if append_eos_token:\n",
        "            tokens.append(self.eos_token)\n",
        "        return tokens\n",
        "\n",
        "    def encode(self, text: str, append_eos_token: bool = True) -> List[str]:\n",
        "        \"\"\" Токенизируем текст \"\"\"\n",
        "        tokens = self._tokenize(text, append_eos_token)\n",
        "        ids = [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, input_ids: Iterable[int], remove_special_tokens: bool = False) -> str:\n",
        "        assert len(input_ids)\n",
        "        assert max(input_ids) < len(self.vocab) and min(input_ids) >= 0\n",
        "        tokens = []\n",
        "        for ind in input_ids:\n",
        "            token = self.inverse_vocab[ind]\n",
        "            if remove_special_tokens and token in self.special_tokens:\n",
        "                continue\n",
        "            tokens.append(token)\n",
        "        text = ' '.join( tokens )\n",
        "        return text\n",
        "\n",
        "    def save(self, path: str) -> bool:\n",
        "        data = {\n",
        "            'token_pattern': self.token_pattern,\n",
        "            'eos_token': self.eos_token,\n",
        "            'pad_token': self.pad_token,\n",
        "            'unk_token': self.unk_token,\n",
        "            'special_tokens': self.special_tokens,\n",
        "            'vocab': self.vocab,\n",
        "            'inverse_vocab': self.inverse_vocab,\n",
        "        }\n",
        "\n",
        "        with open(path, 'wb') as fout:\n",
        "            pickle.dump(data, fout)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def load(self, path: str) -> bool:\n",
        "        with open(path, 'rb') as fin:\n",
        "            data = pickle.load(fin)\n",
        "\n",
        "        self.token_pattern = data['token_pattern']\n",
        "        self.eos_token = data['eos_token']\n",
        "        self.pad_token = data['pad_token']\n",
        "        self.unk_token = data['unk_token']\n",
        "        self.special_tokens = data['special_tokens']\n",
        "        self.vocab = data['vocab']\n",
        "        self.inverse_vocab = data['inverse_vocab']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6999f620",
      "metadata": {
        "id": "6999f620"
      },
      "source": [
        "Класс для задания параметров генерации, так удобней писать логику для валидации параметров и разные другие доп методы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f14d758b",
      "metadata": {
        "id": "f14d758b"
      },
      "outputs": [],
      "source": [
        "class GenerationConfig:\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Тут можно задать любые параметры и их значения по умолчанию\n",
        "        Значения для стратегии декодирования decoding_strategy: ['max', 'top-p']\n",
        "        \"\"\"\n",
        "        self.temperature = kwargs.pop(\"temperature\", 1.0)\n",
        "        self.max_tokens = kwargs.pop(\"max_tokens\", 32)\n",
        "        self.sample_top_p = kwargs.pop(\"sample_top_p\", 0.9)\n",
        "        self.sample_top_k = kwargs.pop(\"sample_top_k\", 1)\n",
        "        self.decoding_strategy = kwargs.pop(\"decoding_strategy\", 'max')\n",
        "        self.remove_special_tokens = kwargs.pop(\"remove_special_tokens\", False)\n",
        "        self.validate()\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\" Здесь можно валидировать параметры \"\"\"\n",
        "        if not (1.0 > self.sample_top_p > 0):\n",
        "            raise ValueError('sample_top_p')\n",
        "        if self.decoding_strategy not in ['max', 'top-p', 'top-k']:\n",
        "            raise ValueError('decoding_strategy')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bd923a4",
      "metadata": {
        "id": "6bd923a4"
      },
      "source": [
        "Сама LM на основе n-грамм. Тут используется сглаживание Лапласа (можно поменять на метод backoff при желании), а также есть ряд параметров, сильно влияющий на качество генерации. Один из параметров генерации - стратегия генерации.\n",
        "\n",
        "Когда мы получили вероятности для следующего токена, мы по этим вероятностям хотим выбрать этот следующий токен.\n",
        "\n",
        "Можно просто семплировать из этого распределения - но тогда есть шанс, что будут очень маловероятные токены.\n",
        "\n",
        "Можно брать самый вероятный токен - но это плохо повлияет на разнообразие и \"человечность\" языка\n",
        "\n",
        "Можно воспользовать подходом top-p - семплировать только из тех токенов, которые наиболее вероятны (их вероятности суммируются в заданный p)\n",
        "\n",
        "Можно проверить, что top-p будет генерировать более интересный текст чем max\n",
        "\n",
        "Также обратите внимание на параметр температуры. В случае top-p и семплирования, чем больше делаешь температуру, тем меньше отличаются друг от друга вероятности (распределение стремится к равномерному, даже если исходное распределение имело вполне себе выраженные максимумы), и текст становится более случайным (и разнообразным)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "462316de",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T15:03:21.268282Z",
          "iopub.status.busy": "2024-02-21T15:03:21.267567Z",
          "iopub.status.idle": "2024-02-21T15:03:21.292501Z",
          "shell.execute_reply": "2024-02-21T15:03:21.291323Z",
          "shell.execute_reply.started": "2024-02-21T15:03:21.268251Z"
        },
        "id": "462316de"
      },
      "outputs": [],
      "source": [
        "class StatLM:\n",
        "    def __init__(self,\n",
        "                 tokenizer: Tokenizer,\n",
        "                 context_size: int = 2,\n",
        "                 alpha: float = 0.1\n",
        "                ):\n",
        "\n",
        "        assert context_size >= 2\n",
        "\n",
        "        self.context_size = context_size\n",
        "        self.tokenizer = tokenizer\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.n_gramms_stat = defaultdict(int)\n",
        "        self.nx_gramms_stat = defaultdict(int)\n",
        "\n",
        "    # def get_token_by_ind(self, ind: int) -> str:\n",
        "    #     return self.tokenizer.vocab.get(ind)\n",
        "\n",
        "    # def get_ind_by_token(self, token: str) -> int:\n",
        "    #     return self.tokenizer.inverse_vocab.get(token, self.tokenizer.inverse_vocab[self.tokenizer.unk_token])\n",
        "\n",
        "    def train(self, train_texts: List[str]):\n",
        "        for sentence in tqdm(train_texts, desc='train lines'):\n",
        "            sentence_ind = self.tokenizer.encode(sentence)\n",
        "            for i in range(len(sentence_ind) - self.context_size + 1):\n",
        "\n",
        "                seq = tuple(sentence_ind[i: i + self.context_size - 1])\n",
        "                self.n_gramms_stat[seq] += 1\n",
        "\n",
        "                seq_x = tuple(sentence_ind[i: i + self.context_size])\n",
        "                self.nx_gramms_stat[seq_x] += 1\n",
        "\n",
        "            # seq = tuple(sentence_ind[len(sentence_ind) - self.context_size:])\n",
        "            # self.n_gramms_stat[seq] += 1\n",
        "\n",
        "    def sample_token(self,\n",
        "                     token_distribution: np.ndarray,\n",
        "                     generation_config: GenerationConfig) -> int:\n",
        "        if generation_config.decoding_strategy == 'max':\n",
        "            return token_distribution.argmax()\n",
        "        elif generation_config.decoding_strategy == 'top-p':\n",
        "            token_distribution = sorted(list(zip(token_distribution, np.arange(len(token_distribution)))),\n",
        "                                        reverse=True)\n",
        "            total_proba = 0.0\n",
        "            tokens_to_sample = []\n",
        "            tokens_probas = []\n",
        "            for token_proba, ind in token_distribution:\n",
        "                tokens_to_sample.append(ind)\n",
        "                tokens_probas.append(token_proba)\n",
        "                total_proba += token_proba\n",
        "                if total_proba >= generation_config.sample_top_p:\n",
        "                    break\n",
        "            # для простоты отнормируем вероятности, чтобы суммировались в единицу\n",
        "            tokens_probas = np.array(tokens_probas) / generation_config.temperature\n",
        "            tokens_probas = tokens_probas / tokens_probas.sum()\n",
        "            return np.random.choice(tokens_to_sample, p=tokens_probas)\n",
        "        elif generation_config.decoding_strategy == 'top-k':\n",
        "            tokens_to_sample = token_distribution.argsort()[-generation_config.sample_top_k: ]\n",
        "            tokens_probas = token_distribution[tokens_to_sample]\n",
        "            tokens_probas = tokens_probas / tokens_probas.sum()\n",
        "            return np.random.choice(tokens_to_sample, p=tokens_probas)\n",
        "        else:\n",
        "            raise ValueError(f'Unknown decoding strategy: {generation_config.decoding_strategy}')\n",
        "\n",
        "    def save_stat(self, path: str) -> bool:\n",
        "        stat = {\n",
        "            'n_gramms_stat': self.n_gramms_stat,\n",
        "            'nx_gramms_stat': self.nx_gramms_stat,\n",
        "            'context_size': self.context_size,\n",
        "            'alpha': self.alpha\n",
        "        }\n",
        "        with open(path, 'wb') as fout:\n",
        "            pickle.dump(stat, fout)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def load_stat(self, path: str) -> bool:\n",
        "        with open(path, 'rb') as fin:\n",
        "            stat = pickle.load(fin)\n",
        "\n",
        "        self.n_gramms_stat = stat['n_gramms_stat']\n",
        "        self.nx_gramms_stat = stat['nx_gramms_stat']\n",
        "        self.context_size = stat['context_size']\n",
        "        self.alpha = stat['alpha']\n",
        "\n",
        "        return True\n",
        "\n",
        "    def get_stat(self) -> Dict[str, Dict]:\n",
        "\n",
        "        n_token_stat, nx_token_stat = {}, {}\n",
        "        for token_inds, count in self.n_gramms_stat.items():\n",
        "            n_token_stat[self.tokenizer.decode(token_inds)] = count\n",
        "\n",
        "        for token_inds, count in self.nx_gramms_stat.items():\n",
        "            nx_token_stat[self.tokenizer.decode(token_inds)] = count\n",
        "\n",
        "        return {\n",
        "            'n gramms stat': self.n_gramms_stat,\n",
        "            'n+1 gramms stat': self.nx_gramms_stat,\n",
        "            'n tokens stat': n_token_stat,\n",
        "            'n+1 tokens stat': nx_token_stat,\n",
        "        }\n",
        "\n",
        "    def _get_next_token(self,\n",
        "                        tokens: List[int],\n",
        "                        generation_config: GenerationConfig) -> (int, str):\n",
        "        denominator = self.n_gramms_stat.get(tuple(tokens), 0) + self.alpha * len(self.tokenizer.vocab)\n",
        "        numerators = []\n",
        "        for ind in self.tokenizer.inverse_vocab:\n",
        "            numerators.append(self.nx_gramms_stat.get(tuple(tokens + [ind]), 0) + self.alpha)\n",
        "\n",
        "        token_distribution = np.array(numerators) / denominator\n",
        "        max_proba_ind = self.sample_token(token_distribution, generation_config)\n",
        "\n",
        "        n = 11\n",
        "        top_n_token_ind = np.argsort(token_distribution)[-1: -n: -1]\n",
        "        # top_n_token_probas = token_distribution[top_n_token_ind]\n",
        "        # top_n_tokens = [self.tokenizer.inverse_vocab[ind] for ind in top_n_token_ind]\n",
        "        print(f'\\nCurrent context: {[self.tokenizer.inverse_vocab[ind] for ind in tokens]}')\n",
        "        for ind in top_n_token_ind:\n",
        "            print(f'{self.tokenizer.inverse_vocab[ind]}: {token_distribution[ind]}')\n",
        "\n",
        "        next_token = self.tokenizer.inverse_vocab[max_proba_ind]\n",
        "        print(f'Next token: {next_token}')\n",
        "\n",
        "        return max_proba_ind, next_token\n",
        "\n",
        "    def generate_token(self,\n",
        "                       text: str,\n",
        "                       generation_config: GenerationConfig\n",
        "                      ) -> Dict:\n",
        "        tokens = self.tokenizer.encode(text, append_eos_token=False)\n",
        "        tokens = tokens[-self.context_size + 1:]\n",
        "\n",
        "        max_proba_ind, next_token = self._get_next_token(tokens, generation_config)\n",
        "\n",
        "        return {\n",
        "            'next_token': next_token,\n",
        "            'next_token_num': max_proba_ind,\n",
        "        }\n",
        "\n",
        "\n",
        "    def generate_text(self, text: str,\n",
        "                      generation_config: GenerationConfig\n",
        "                     ) -> Dict:\n",
        "\n",
        "        all_tokens = self.tokenizer.encode(text, append_eos_token=False)\n",
        "        tokens = all_tokens[-self.context_size + 1:]\n",
        "\n",
        "        next_token = None\n",
        "        while next_token != self.tokenizer.eos_token and len(all_tokens) < generation_config.max_tokens:\n",
        "            max_proba_ind, next_token = self._get_next_token(tokens, generation_config)\n",
        "            all_tokens.append(max_proba_ind)\n",
        "            tokens = all_tokens[-self.context_size + 1:]\n",
        "\n",
        "        new_text = self.tokenizer.decode(all_tokens, generation_config.remove_special_tokens)\n",
        "\n",
        "        finish_reason = 'max tokens'\n",
        "        if all_tokens[-1] == self.tokenizer.vocab[self.tokenizer.eos_token]:\n",
        "            finish_reason = 'end of text'\n",
        "\n",
        "        return {\n",
        "            'all_tokens': all_tokens,\n",
        "            'total_text': new_text,\n",
        "            'finish_reason': finish_reason\n",
        "        }\n",
        "\n",
        "    def generate(self, text: str, generation_config: Dict) -> str:\n",
        "        return self.generate_text(text, generation_config)['total_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90f60f19",
      "metadata": {
        "id": "90f60f19"
      },
      "source": [
        "Эта функция напрямую используется в телеграм боте для получения модели и конфига генерации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cfdad05",
      "metadata": {
        "id": "4cfdad05"
      },
      "outputs": [],
      "source": [
        "def construct_model():\n",
        "    config = {\n",
        "        'temperature': 1.0,\n",
        "        'max_tokens': 32,\n",
        "        'sample_top_p': 0.9,\n",
        "        'decoding_strategy': 'top-p',\n",
        "    }\n",
        "\n",
        "    stat_lm_path = 'models/stat_lm/stat_lm.pkl'\n",
        "    tokenizer_path = 'models/stat_lm/tokenizer.pkl'\n",
        "\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.load(tokenizer_path)\n",
        "\n",
        "    stat_lm = StatLM(tokenizer)\n",
        "    stat_lm.load_stat(stat_lm_path)\n",
        "\n",
        "    generation_config = GenerationConfig(temperature=config['temperature'],\n",
        "                                         max_tokens=config['max_tokens'],\n",
        "                                         sample_top_p=config['sample_top_p'],\n",
        "                                         decoding_strategy=config['decoding_strategy'],\n",
        "                                         remove_special_tokens=True)\n",
        "\n",
        "    kwargs = {'generation_config': generation_config}\n",
        "    return stat_lm, kwargs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a85b767",
      "metadata": {
        "id": "3a85b767"
      },
      "source": [
        "### Обучаем на игрушечных данных"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2674d63a",
      "metadata": {
        "id": "2674d63a"
      },
      "source": [
        "Для демонстрации того, что происходит, возьмем несколько коротких цитат Джейсона Стэтхема отсюда:\n",
        "\n",
        "https://dzen.ru/a/ZRFaGN_gKhX6xTWW"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKyrhiTiUtYZ",
        "outputId": "431b899d-6b36-40ce-b0a9-a179e7ab3616"
      },
      "id": "oKyrhiTiUtYZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fa25b3b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T15:03:23.642385Z",
          "iopub.status.busy": "2024-02-21T15:03:23.641563Z",
          "iopub.status.idle": "2024-02-21T15:03:23.647640Z",
          "shell.execute_reply": "2024-02-21T15:03:23.646711Z",
          "shell.execute_reply.started": "2024-02-21T15:03:23.642338Z"
        },
        "id": "7fa25b3b"
      },
      "outputs": [],
      "source": [
        "# texts = [\n",
        "#     'Взял нож - режь, взял дошик - ешь.',\n",
        "#     'Никогда не сдавайтесь, идите к своей цели! А если будет сложно – сдавайтесь.',\n",
        "#     'Запомни: всего одна ошибка – и ты ошибся.',\n",
        "#     'В жизни всегда есть две дороги: одна — первая, а другая — вторая.',\n",
        "#     'Делай, как надо. Как не надо, не делай.',\n",
        "#     'Работа не волк. Никто не волк. Только волк волк.',\n",
        "#     'Работа не волк. Работа - ворк. А волк - это ходить.',\n",
        "#     'Работа',\n",
        "#     ]\n",
        "\n",
        "# train_texts = texts[:-1]\n",
        "# test_text = texts[-1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_2ch_dialogues():\n",
        "    df = pd.read_csv('/content/drive/MyDrive/NLP_2024/assistant/datasets/2ch_dialogues.tsv', sep='\\t')\n",
        "    texts = []\n",
        "    for i in tqdm(range(df.shape[0])):\n",
        "        message = df.iloc[i].message\n",
        "        if len(message.split()) < 2:\n",
        "            continue\n",
        "        # message = re.sub('\\W', ' ', message)\n",
        "        texts.append(message)\n",
        "    return texts\n",
        "\n",
        "\n",
        "# def get_dialogues():\n",
        "#     texts = []\n",
        "#     with open('/content/drive/MyDrive/NLP_2024/assistant/datasets/dialogues.txt', 'r') as f:\n",
        "#         prev_line = None\n",
        "#         for line in f.readlines():\n",
        "#             if line == '\\n':\n",
        "#                 prev_line = None\n",
        "#                 continue\n",
        "#             assert line.startswith('-')\n",
        "#             line = line.strip(' -\\n')\n",
        "#             if prev_line is None:\n",
        "#                 prev_line = line\n",
        "#                 continue\n",
        "#             text = prev_line + ' <PAD> ' + line\n",
        "#             texts.append(text)\n",
        "#             prev_line = line\n",
        "#     return texts\n",
        "\n",
        "def get_dialogues():\n",
        "    texts = []\n",
        "    with open('/content/drive/MyDrive/NLP_2024/assistant/datasets/dialogues.txt', 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            if len(line.split()) < 3:\n",
        "                continue\n",
        "            assert line.startswith('-')\n",
        "            line = line.strip(' -\\n')\n",
        "            texts.append(line)\n",
        "    return texts\n",
        "\n",
        "def get_dialogues_2():\n",
        "    dataset = load_dataset(\"Den4ikAI/russian_dialogues_2\")\n",
        "    texts = []\n",
        "    for sample in dataset['train']['sample']:\n",
        "        texts += eval(sample)\n",
        "    return texts\n",
        "\n",
        "\n",
        "def get_movie_dialogues():\n",
        "    dataset = load_dataset(\"rcp-meetings/rudialogsum_v2\")\n",
        "    dialogues = dataset['train']['dialog']\n",
        "    dialogues = [re.sub('<\\w+>', ' ', d) for d in dialogues]\n",
        "    summaries = dataset['train']['summary']\n",
        "    summaries = [re.sub('<\\w+>', ' ', s) for s in summaries]\n",
        "    return dialogues + summaries\n",
        "\n",
        "\n",
        "def get_jokes():\n",
        "    dataset = load_dataset(\"IgorVolochay/russian_jokes\")\n",
        "    return dataset['train']['text']"
      ],
      "metadata": {
        "id": "IT7s1NldKsyt"
      },
      "id": "IT7s1NldKsyt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dialogues_2 = get_dialogues_2()\n",
        "dialogues_2 = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko3nc2go_INL",
        "outputId": "0a3fd311-f5cf-4368-ca9e-fdd587cb705a"
      },
      "id": "Ko3nc2go_INL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dvach_dialogues = get_2ch_dialogues()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0be444ba6d9e436e8b1b6613b330e360",
            "220053eba753436f807e2b4c11e8207f",
            "b7bc33f52b7247c9bb587e0ec292183f",
            "a294b6bc52e04a33a1beaa815bbb5555",
            "84ac2a43a6f4494783498fa271525d47",
            "ef98149c7a004ad6a9bd73e5ed9d925f",
            "523d6e68c53543bb98458e99eb724eb3",
            "26f761d8d7c0497d9a498990f26085e8",
            "9fb6b40036ed45be8f4af8c4e4ae6bcc",
            "fc870ed4f633456bb88de03f3310f2e1",
            "0aaa288f728d46439a7c0613e89cb913"
          ]
        },
        "id": "kOoSS-qjupdT",
        "outputId": "a1fc927c-2948-481e-8760-dc7d1efae312"
      },
      "id": "kOoSS-qjupdT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/582617 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0be444ba6d9e436e8b1b6613b330e360"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dialogues = get_dialogues()"
      ],
      "metadata": {
        "id": "I4v_C0NZuyzg"
      },
      "id": "I4v_C0NZuyzg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jokes = get_jokes()"
      ],
      "metadata": {
        "id": "4UqvC6szu200"
      },
      "id": "4UqvC6szu200",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_dialogues = get_movie_dialogues()"
      ],
      "metadata": {
        "id": "f2uSCEdHu5Zy"
      },
      "id": "f2uSCEdHu5Zy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts = dialogues_2 + dvach_dialogues + dialogues + jokes + movie_dialogues\n",
        "len(train_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "kP6bZM7DMzm1",
        "outputId": "a6872722-e19b-43ad-d95c-2581b45d340c"
      },
      "id": "kP6bZM7DMzm1",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dialogues_2' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b2cc0f7704eb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdialogues_2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdvach_dialogues\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdialogues\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mjokes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmovie_dialogues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# train_texts = jokes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dialogues_2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nrCp098QJSY",
        "outputId": "e322b265-8675-4184-ad90-98f33894ed70"
      },
      "id": "6nrCp098QJSY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Катали на такой. Свободного ляма говоришь не было? А ежемесячно на заправки сколько бы уходило не прикидывал? ...',\n",
              " 'а расход у неё какой?',\n",
              " 'Написано 290л бак, запас хода 750км, ну в среднем 39л на сотню',\n",
              " 'Карб от Москвича и норм :. ',\n",
              " 'Можно же сделать на один глаз, не? Нафига тебе супер-зрение на обоих?',\n",
              " 'А потом ходить так? ..',\n",
              " 'У меня повышенная светобоязнь. Если на улице ярко - двумя глазами смотреть невозможно. Поэтому примерно так и хожу :C',\n",
              " 'А в чем причина? Ходил к врачу?',\n",
              " 'Ставят коньюктивит. Хотя глаза не болят, не слезятся и не гноятся. Наследственность от отца, у сестры тоже такое есть, но в меньшей степени',\n",
              " 'Может ты ещё и баночку со звёздочкой с первого раза открываешь? Что ты такое?']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98e92a50",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T15:03:25.218697Z",
          "iopub.status.busy": "2024-02-21T15:03:25.218225Z",
          "iopub.status.idle": "2024-02-21T15:03:25.223271Z",
          "shell.execute_reply": "2024-02-21T15:03:25.222261Z",
          "shell.execute_reply.started": "2024-02-21T15:03:25.218665Z"
        },
        "id": "98e92a50"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer().build_vocab(train_texts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzU9Z2cOvYEF",
        "outputId": "39ec3658-4577-433d-fd44-26c906cf6eb4"
      },
      "id": "JzU9Z2cOvYEF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1423386"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f61a4e82",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T15:03:25.398291Z",
          "iopub.status.busy": "2024-02-21T15:03:25.397981Z",
          "iopub.status.idle": "2024-02-21T15:03:25.406154Z",
          "shell.execute_reply": "2024-02-21T15:03:25.405134Z",
          "shell.execute_reply.started": "2024-02-21T15:03:25.398265Z"
        },
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f61a4e82",
        "outputId": "825bb418-2b13-493e-d08f-72a58d9af4c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'уасями': 0, 'торму': 1, 'софоны': 2, 'кт': 3, 'петёфи': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "dict(list(tokenizer.vocab.items())[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ef0948d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T15:03:29.936948Z",
          "iopub.status.busy": "2024-02-21T15:03:29.936563Z",
          "iopub.status.idle": "2024-02-21T15:03:29.957657Z",
          "shell.execute_reply": "2024-02-21T15:03:29.956641Z",
          "shell.execute_reply.started": "2024-02-21T15:03:29.936919Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "162482f64308405b9f67c2d148ec17da",
            "8176999d788b4a44ae86d2227bea2019",
            "7004fede1bf14edcaa07ae982054c280",
            "c22c345c6b5d4b98af8bc34dc13d23b1",
            "b9152191b865498cbb443306b1b67822",
            "4cc9f2b3c8ec4548935968324d876df7",
            "122998fd7864475ba4f918fe89dce105",
            "fac15eb65d934dd68acdb0daeea7865a",
            "d9724246cbb44a16aea197f707b0592a",
            "526fd495525848fdbfc0ac1a30a65d45",
            "4ba5dc73c3de4b1b97bc7f170f4033dd"
          ]
        },
        "id": "2ef0948d",
        "outputId": "4a76642e-1609-4558-9174-fe06c5903115"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train lines:   0%|          | 0/10927866 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "162482f64308405b9f67c2d148ec17da"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# класс, который позволяем строить и использовать языковую модель на основе n-грамм\n",
        "stat_lm = StatLM(tokenizer, context_size=3, alpha=0.1) # , sample_top_p = None\n",
        "\n",
        "# \"обучаем\" модель - считаем статистики\n",
        "stat_lm.train(train_texts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig(\n",
        "    temperature = 1.0,\n",
        "    max_tokens = 32,\n",
        "    sample_top_p = 0.001,\n",
        "    sample_top_k = 4,\n",
        "    decoding_strategy = 'max',\n",
        "    remove_special_tokens=True\n",
        ")"
      ],
      "metadata": {
        "id": "IPorXeINhH5m"
      },
      "id": "IPorXeINhH5m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stat_lm.generate(\"я бы так не сказал\", generation_config))"
      ],
      "metadata": {
        "id": "1EoCvk6WsiBU"
      },
      "id": "1EoCvk6WsiBU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stat_lm.generate(\"хочу шоколадку\", generation_config))"
      ],
      "metadata": {
        "id": "c9DCh80Us0yh"
      },
      "id": "c9DCh80Us0yh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stat_lm.generate(\"я сегодня дома\", generation_config))"
      ],
      "metadata": {
        "id": "BTgG9z-_r6LL"
      },
      "id": "BTgG9z-_r6LL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stat_lm.generate(\"я тебя найду\", generation_config))"
      ],
      "metadata": {
        "id": "phkIncMkrt9p"
      },
      "id": "phkIncMkrt9p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e73f538",
      "metadata": {
        "id": "1e73f538"
      },
      "outputs": [],
      "source": [
        "print(stat_lm.generate(\"у меня дошик\", generation_config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d054a1d4",
      "metadata": {
        "id": "d054a1d4"
      },
      "outputs": [],
      "source": [
        "tokenizer.save('/content/drive/MyDrive/NLP_2024/assistant/models/stat_lm/tokenizer_large.pkl')\n",
        "stat_lm.save_stat('/content/drive/MyDrive/NLP_2024/assistant/models/stat_lm/stat_lm_large.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "600bcef7",
      "metadata": {
        "id": "600bcef7"
      },
      "source": [
        "Тут мы для токенизатора сохраняем только спецтокены и словарь, для модели - параметры и статистики n-грамм и n+1-грамм. Потом в телеграм боте подгружаем именно эти параметры"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dbf63a9",
      "metadata": {
        "id": "1dbf63a9"
      },
      "source": [
        "Когда обучите модель на большом датасете, советую посмотреть на распределение вероятностей для следующего слова при разных входах"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e659ff6",
      "metadata": {
        "id": "0e659ff6"
      },
      "source": [
        "### смотрим как конструировать"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ad87959",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "0ad87959",
        "outputId": "dc4c679a-0475-4118-a012-1a5d851e0f3e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'models/stat_lm/tokenizer.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-e38418d122f0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"у меня дошик\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-5411c4ee8794>\u001b[0m in \u001b[0;36mconstruct_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mstat_lm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStatLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-5d11160b6a08>\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/stat_lm/tokenizer.pkl'"
          ]
        }
      ],
      "source": [
        "model, kwargs = construct_model()\n",
        "\n",
        "model.generate(\"у меня дошик\", **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "517748d6",
      "metadata": {
        "id": "517748d6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30648,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0be444ba6d9e436e8b1b6613b330e360": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_220053eba753436f807e2b4c11e8207f",
              "IPY_MODEL_b7bc33f52b7247c9bb587e0ec292183f",
              "IPY_MODEL_a294b6bc52e04a33a1beaa815bbb5555"
            ],
            "layout": "IPY_MODEL_84ac2a43a6f4494783498fa271525d47"
          }
        },
        "220053eba753436f807e2b4c11e8207f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef98149c7a004ad6a9bd73e5ed9d925f",
            "placeholder": "​",
            "style": "IPY_MODEL_523d6e68c53543bb98458e99eb724eb3",
            "value": "100%"
          }
        },
        "b7bc33f52b7247c9bb587e0ec292183f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26f761d8d7c0497d9a498990f26085e8",
            "max": 582617,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9fb6b40036ed45be8f4af8c4e4ae6bcc",
            "value": 582617
          }
        },
        "a294b6bc52e04a33a1beaa815bbb5555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc870ed4f633456bb88de03f3310f2e1",
            "placeholder": "​",
            "style": "IPY_MODEL_0aaa288f728d46439a7c0613e89cb913",
            "value": " 582617/582617 [01:20&lt;00:00, 8132.81it/s]"
          }
        },
        "84ac2a43a6f4494783498fa271525d47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef98149c7a004ad6a9bd73e5ed9d925f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "523d6e68c53543bb98458e99eb724eb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26f761d8d7c0497d9a498990f26085e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fb6b40036ed45be8f4af8c4e4ae6bcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc870ed4f633456bb88de03f3310f2e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0aaa288f728d46439a7c0613e89cb913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "162482f64308405b9f67c2d148ec17da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8176999d788b4a44ae86d2227bea2019",
              "IPY_MODEL_7004fede1bf14edcaa07ae982054c280",
              "IPY_MODEL_c22c345c6b5d4b98af8bc34dc13d23b1"
            ],
            "layout": "IPY_MODEL_b9152191b865498cbb443306b1b67822"
          }
        },
        "8176999d788b4a44ae86d2227bea2019": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cc9f2b3c8ec4548935968324d876df7",
            "placeholder": "​",
            "style": "IPY_MODEL_122998fd7864475ba4f918fe89dce105",
            "value": "train lines:   2%"
          }
        },
        "7004fede1bf14edcaa07ae982054c280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fac15eb65d934dd68acdb0daeea7865a",
            "max": 10927866,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9724246cbb44a16aea197f707b0592a",
            "value": 228801
          }
        },
        "c22c345c6b5d4b98af8bc34dc13d23b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_526fd495525848fdbfc0ac1a30a65d45",
            "placeholder": "​",
            "style": "IPY_MODEL_4ba5dc73c3de4b1b97bc7f170f4033dd",
            "value": " 228801/10927866 [00:18&lt;09:13, 19346.24it/s]"
          }
        },
        "b9152191b865498cbb443306b1b67822": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cc9f2b3c8ec4548935968324d876df7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "122998fd7864475ba4f918fe89dce105": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fac15eb65d934dd68acdb0daeea7865a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9724246cbb44a16aea197f707b0592a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "526fd495525848fdbfc0ac1a30a65d45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ba5dc73c3de4b1b97bc7f170f4033dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}